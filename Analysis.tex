\documentclass[12pt]{article}

\usepackage{algpseudocode,algorithm,algorithmicx}
\usepackage{amsmath}
\usepackage[margin=0.92in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}

\pagestyle{fancy}
\pagenumbering{arabic}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
  }

\title{Graph Algrithms}
\author{Anas Rchid}
\date{March 20, 2020}

\newcommand*\Let[2]{\State #1 $\gets$ #2}

\begin{document}
\maketitle
\vspace*{\fill}
\begin{quotation}
  \begin{center}
    \section*{Abstract}
  \end{center}
  This is a solution to a graph algorithms assignment, it has three problems with two objectives. The problems are generally common in graph theory, being \textit{strongly connected components}, \textit{path finding} and \textit{vertices ranking}. Solving those problems by a mere implementation is not enough, thus the existence of this paper. It aims to explain some theoretical details about the implementations and the reason behind some decisions. \\

  Although the main reason behind it is to test my abilities to describe what I have learned formally, and it would do no harm to write everything down and share it with everybody who's willing to learn about those algorithm.
\end{quotation}
\vspace*{\fill}
\clearpage

\tableofcontents
\vspace*{\fill}
\listofalgorithms
\vspace*{\fill}
\clearpage

\section{Graphs and Graph Algorithms}

A graph $G(V,E)$ is a pair of sets $V$ and $E$, being the set of vertices and the set edges, respectively. Each edge has linked two vertices such that one is a source and the other is a destination.

There are many graph algorithms, and depending on the type of the graph, some might be applicable, others are not.  Generally, graphs have two distinct properties, namely \textit{edge direction} and \textit{edge weight}. If edges are directed that means we can only traverse in the spesified direction, as oppose to undirected edges. Also the weight could be any real value --- edges could have negative weight! Thus we have four possibilities:

\begin{itemize}
\item being undirected and either unweighted or weighted
\item being directed and either unweighted or weighted
\end{itemize}

\subsection{Representing a Graph}

Graph could be represented in many ways, depending on whether the graph in question is a \textit{dense} of a \textit{spare} graph --- the number of edges relative to the maximum number of possible edges in a graph. The main three ways of representing a graph are \textit{Adjacency Lists}, \textit{Adjacency Matrices} and \textit{Edge list}. Where all of them provide the same thing, which is a repsentation of the graph, their usage cases might vary.

\paragraph{Adjacency Matrix}

The adjacency matrix is a matrix that hold information about on each pair of vertices in the graph --- whether the have an edge or not. It's suitable for both directed an undirected graphs, and especially convenient for dense graphs since it provides the ability to check adjacency in $O(1)$. However, this comes with a cost of $O(V^2)$ space complexity. So, unless the graph is dense, this should be avoided in order to avoid wasting memory.

Given a directed graph $G(V, E)$, the adjacency matrix $M$ is a matrix of squared matrix of size $|V|$ where if $M_{uv} = 1$ then $(u,v) \in E$. \[
  \textit{if } V=\{1,2,3,4,5\} \textit{ and } E=\left\{  \begin{matrix}
      (1,2) & (1,3) & (1,4) \\
      (2,1) & (2,3) & (3,4) \\
      (4,1) & (4,3) & (5,1) \\
      (5,2) & (5,3) & (5,4)
    \end{matrix}
  \right\}  \textit{ then we have }
  M_G =
  \begin{bmatrix}
    0 & 1 & 1 & 1 & 0 \\
    1 & 0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 1 & 0 & 0 \\
    1 & 1 & 1 & 1 & 0 \\
  \end{bmatrix}
\]

\paragraph{Adjacency List}
Adjacency list are the general way to represent the graph since their light weight and the way to go for spare graphs. The thing is that we represent the graph by building an array of lists, where each list represent all the neighbors of a vertex's. Thus for the same graph above, we'll have the following adjacency list \[
  L_G = \left\{
    \begin{matrix}
      \{2,3,4\} \\
      \{1,3\} \\
      \{4\} \\
      \{1,3\} \\
      \{1,2,3,4\}
    \end{matrix}
  \right\}
\]

But, unlike the adjancecy matrix, we need $O(E)$ to check if an edge exists between two vertices. However, we only need $O(E+V)$ to store the list.

\subsection{Traversing a graph}

\subsubsection{Breath First Search}

Along with Depth First Search, Breath First Search is a fundamental graph traversal algorithm. Both resemble different approaches of how to traverse a given graph. BFS, works only on \textit{unweighted} graphs, and this is actually not a bad thing, matter of fact, BFS is a sub-routine in other more sophisticated algorithms. BFS traverses the graph is a \textit{layered} fashion, meaning that each time we move through the whole graph level by level, starting at a given vertex, and making our way through, until we visit all other reachable vertices.

\input{docs/bfs.tex}

\paragraph{Algorithm details} Given a graph $G(V,E)$, a vertex $s \in G$ and a queue $Q$. We start by enqueuing the $s$ and mark it as visited, so that we can ignore it later if we reached it from another path. Then while $Q$ is not empty, we enqueue all destination vertices of $s$, after marking them too as visited. Repeating this process with each vertex in $Q$ ensure that we'll traverse the whole graph as layers. However, if a destination vertex is specified, we might stop as soon as it's dequeued.


\paragraph{Time Complexity} First of all, we shall traverse all the vertices, $O(V)$, and we follow all the edges\footnote{Graphs can be sparse or dense, thus $O(E)$ may raise up to $O(V^2)$} $O(E)$. Thus the total time complexity of the Breath First Search is $O(V+E)$.

\paragraph{Space Complexity} The queue used to track which vertices to visit next consume a space complexity of $O(V)$ at the worst case, since we might store all vertices in the queue at once.

\paragraph{Applications} As mentioned above, BFS finds the shortest path within  an unweighted graph, in terms of number of edges. Also, it is a sub-routine in Ford Fulkerson Method to find the maximum flow in a given graph. And due to its nature, BFS makes it also perfect as well for finding near neighbors\footnote{Since we traverse the graph layer at a time, and in all directions}, which is really useful to know about networks and navigation's systems, rather than using it to find whether a graph is Bipartite\footnote{A graph that it's vertices set could be splited into two sets, such that there is no edge connecting two vertices in the same set} or not.

\subsubsection{Depth First Search}
\paragraph{Algorithm details}
\paragraph{Time Complexity}
\paragraph{Space Complexity}
\paragraph{Applications}

\section{Path Finding}

A path is a list of vertices that are connect successively via edges, and lead from a source vertex \textit{s} to a destination vertex \textit{t}. In other words, we have a recursive dependecy between each vertex and its parent, say we have an array $p$ that hold each vertex's parent, thus if $s$ and $t$ are connext via a path $\omega$, then \[
  \omega = \left\{s, \ldots, p[p[p[t]]] ,p[p[t]],p[t], t\right\}
\]

\input{docs/path.tex}

More specifically, a \textit{shortest path} is a path which has the \textit{minimal cost}, there could be multiple shortest paths in the same graph, but by definition all of them must have the same cost. That cost could be either in terms of \textit{number of edges} for an unweighted graph, or the \textit{sum of weights} for a weighted graph.

\subsection{Cycles and Weights}

\subsection{Bellmanâ€“Ford algorithm}

\subsection{Dijkstra's Algorithm}

Dijkstra is a brilliant computer scientist, he has invented many things in the field and his \textit{shortest path algorithm} happens to be so efficient that it becomes sort of a standard when it comes to shortest paths\footnote{Shortest here means minimum cost in terms of the total edge weight between two vertices}. The main idea of Dijkstra is to be as greedy as possible, by always taking the most promising edge, in terms of the total edge weight. This is done by maintaining an array of minimal distances, as well as a picking the next most promising vertex from an open set. However, the graph must no contain any \textit{negative edge weights}, since unlike

\paragraph{Algorithm Details} Given a graph $G(V,E)$, a starting vertex $s$, an array $d$, which stores the \textit{minimum distance} between $s$ and all the other vertices in the graph. The idea is to start at $s$, then for all its adjacent vertices $n_i$, we shall compute their overall distance from $s$, and only update theirs if it's smaller that what it was --- initially we set all vertices to be $\infty$ far away. If so, then we apply a relaxation by updating the minimal distance, then mark it as ``to be visited later'' by storing it into some data structure, ordered by least distance. We keep repeating the same process by picking the next vertex --- the one with the smallest distance so far, until we have no more vertices to visit. The algorithm guarantees that we'll have computed the minimum distance between each pair of vertices in $G$ once it terminates. We can stop early if a destination vertex is given, since it's distance would not change once reached even we can still discover another vertices.

\begin{quotation}
The critical aspect of the algorithm's performance is \textit{how to pick those promising vertices} --- what data structure to use? As it turns out, the more we use a sophisticated data structure, the more we reduce complexity of this process. However, and independently from what data structure is used, we still have to traverse $O(V)$ vertices which is implicitly $O(V)$ queries to get the next promising vertex, as well as performing $O(E)$ updates\footnote{Updates include insertions, since we assume that initially all vertices a cost of $\infty$} in the worst case resulting $O(nE + mV)$ where $n$ is the cost for updating an entry and $m$ is the cost of retrieving an entry. Below are three different approaches that have different complexities relative to their data structures.
\end{quotation}

\paragraph{Space Complexity} Moreover, and independently from the way we represent the graph --- either using an adjacency matrix or adjencency list, we still have to store $O(V)$ distances. However, th space comlexity vary according to which priority queue implementation we've used. It's $O(V)$ for unsorted arrays, since we can update the entry's cost in-place\footnote{This is also the case for Fibonacci Heaps, as well as the Ordered Set tweak}, but it's $O(E)$ if we have used a binary heap, since we allow out-dated to exists.

\paragraph{Applications}

\subsubsection{The Original idea using unsorted Arrays --- $O(E + V^2)$}

Originally, the algorithm used plain arrays that maintain no order, thus the only way to find the next promising vertex is by lineary check every entry in the array. Although this seems easy to implement, but it is very computationally expensive! Since we have to check each time to see whether we have terminated, and check each time to pick a min.

\input{docs/dijkstra_orig.tex}

This is extremely inefficient since we can update vertices in $O(1)$, but finding the promising vertex would take $O(V)$ since we have move through the entire array to find the promising vertex\footnote{And we cannot maintain a sorted array since we'll have to sort in $O(V\log(V))$ and perform a binary search $O(\log(V))$, thus a linear search is better}. Thus, the wholesome time complexity is $O(E) + O(V^2) = O(E + V^2)$, however for spare graphs, it's safe to assume that $|E| < |V|^2$, thus we can ignore the cost of updates relative to the lookup cost resulting a complexity of $O(V^2)$.

\subsubsection{The Lazy Approach using Priority Queues --- $O(E\log(V) + V\log(V))$}

Instead of an array, we can use binary heap implementation as a \textit{priority queue} $pq$ to help keep picking successive promising vertices ---  vertices with the smallest distance from the $s$. This is done by storing pairs $(v,c)$ of each vertex and its distance.

Beforehand, we set the distance of $s$ to $0$, and enqueue it to $pq$. Then, while $pq$ still has entries, we poll a pair, namely $e$. Then for all the $n_i$ neighbors of $e.v$, let $c$ be as the total distance between $s$ and $n_i$ i.e., the stored distance $d[e.v]$ + $n_i$'s edge weight. If $n_i$ hasn't been reached already, then we enqueue the pair $(n_i,c)$ into $pq$. Otherwise, we only enqueue the pair $(n_i,c)$ \textit{iff} $c < d[n_i]$. If so, we update the distance $d[n_i]$ since we've found a better one. Obviously, this approach might result having some outdated distances of the same vertex, but we can simply ignore those in case we have already a better distance, indeed $d[e.v] < e.c$.

\input{docs/dijkstra_lazy.tex}

 One improvment to make is to replace the linear with some constant time operation, since we always want to pick the next most promising vertex --- the one with the minimum cost, we can use a priority queue, generally implemented as binary heaps. We can now get the next vertex in $O(1)$, however this is a bit misleading since re-balancing\footnote{A binary heap needs to be heapify-ed after each get or insert, in order to reestablish the order or place the element in the correct place, respectively} the priority queue after dequeuing an entry would cost $O(\log(V))$. The same goes when enqueuing an entry.

However, for Dijkstra's algorithm since we have to constently update the cost of vertices, and most standard priority queue implementations does not support a critical operation to acheive such a goal optimally --- \texttt{decreaseKey(e, k)}. Luckily, we can get around this issue by following a \textit{lazy} approach, where each time we find a better entry (a one with a less cost for the same vertex), we enqueue it. It's clear that enqueuing new entries result duplicates\footnote{It's better to enqueue new entries in $O(\log(n))$ then to look up for them in $O(n)$}, although we can easily ignore those outdated costs by comparing the stored distances and move forward only for the least ones, but in dense graphs, this would result a lot of \textit{unnecessary work}.

Thus, the wholesome complexity is $O(E\log(V)) + O(V\log(V)) = O((E + V)\log(V))$. However, it's generally safe to assume that $|V| < |E|$, thus we get a $O(E\log(V))$ time complexity. A neat optimization to get ride of the duplicates caused by the lazy approach, is by instead maintaining an ordered set $A$ of pairs $(v, c)$ --- a vertex $v$ with its minimum distance $c$ that's been found so far. Since $A$ is ordered (by cost $c$ obviously), we can assure that the first element is the most promising. However, when a better cost found for the same vertex, we shall remove\footnote{If this operation is done in $O(1)$ then this would be equivalent to decreasing the key of that vertex} the previous cost.

\subsubsection{The Optimal performance using Fibonacci Heaps --- $O(E + V\log{V})$}

TODO: implement this and add details $O{(E + V\log(V))}_{\textit{\small amortized}}$ since they support \texttt{decreaseKey(e, k)} which is $O{(1)}_{\textit{\small amortized}}$ and \texttt{popmin()} in $(\log(V))$.

\input{docs/dijkstra_optimal.tex}

\subsection{Floydâ€“Warshall algorithm}

\section{Strongly Connected Components}

\subsection{Tarjan's Algorithm}

\input{docs/tarjan.tex}

\paragraph{Algorithm Details}
\paragraph{Time Complexity}
\paragraph{Space Complexity}
\paragraph{Applications}

\subsection{Kosaraju's Algorithm}

\input{docs/tarjan.tex}

\paragraph{Algorithm Details}
\paragraph{Time Complexity}
\paragraph{Space Complexity}
\paragraph{Applications}

\section{Ranking}

\subsection{Intuitive Ranking: based on edge degrees}

\input{docs/edges.tex}

\subsection{Google's PageRank}

\paragraph{Algorithm Details}

\input{docs/pagerank.tex}

\paragraph{Time Complexity}
\paragraph{Space Complexity}
\paragraph{Applications}

\end{document}
